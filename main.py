import os
import random
import time
import requests
import json
import base64
import re
from bs4 import BeautifulSoup
from requests.auth import HTTPBasicAuth

# 1. í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY', '')
WP_USERNAME = os.environ.get('WP_USERNAME', '').strip()
WP_APP_PASSWORD = os.environ.get('WP_APP_PASSWORD', '').replace(' ', '').strip()
WP_BASE_URL = "https://virz.net" 

# í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„¤ì • (Trueë©´ 1ê°œë§Œ ì¦‰ì‹œ ë°œí–‰, Falseë©´ 10ê°œ ëœë¤ ë°œí–‰)
IS_TEST = os.environ.get('TEST_MODE', 'false').lower() == 'true'

class NaverScraper:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ë° ë¸”ë¡œê·¸ ë­í‚¹ ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
        }

    def get_news_ranking(self, section_id):
        url = f"https://news.naver.com/main/ranking/popularDay.naver?sectionId={section_id}"
        try:
            res = requests.get(url, headers=self.headers, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            titles = soup.select(".rankingnews_list .list_title")
            cleaned_titles = []
            for t in titles[:10]:
                text = t.text.strip()
                if ']' in text[:10]:
                    text = text.split(']', 1)[-1].strip()
                cleaned_titles.append(text)
            return cleaned_titles
        except Exception as e:
            print(f"ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ({section_id}): {e}", flush=True)
            return []

    def get_blog_hot_topics(self):
        url = "https://section.blog.naver.com/HotTopicList.naver"
        try:
            res = requests.get(url, headers=self.headers, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            topics = soup.select(".list_hottopic .desc")
            return [topic.text.strip() for topic in topics[:10]]
        except Exception as e:
            print(f"ë¸”ë¡œê·¸ í•«í† í”½ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}", flush=True)
            return []

def generate_content(raw_keyword, category):
    """Gemini APIë¥¼ ì´ìš©í•œ ì œëª©, ë³¸ë¬¸, ìš”ì•½, íƒœê·¸ í†µí•© ìƒì„±"""
    model_id = "gemini-2.5-flash-preview-09-2025"
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_id}:generateContent?key={GEMINI_API_KEY}"
    
    # í˜„ì¬ ë‚ ì§œ ì •ë³´ (ë°°ê²½ ì§€ì‹ìœ¼ë¡œë§Œ ì œê³µ)
    current_date = "2026ë…„ 2ì›” 11ì¼"
    
    system_prompt = f"""ë‹¹ì‹ ì€ {category} ë¶„ì•¼ì˜ ì „ë¬¸ SEO ë¸”ë¡œê±°ì…ë‹ˆë‹¤. 
ì°¸ê³ ìš© í˜„ì¬ ë‚ ì§œëŠ” {current_date}ì…ë‹ˆë‹¤. ì´ ë‚ ì§œëŠ” ì •ë³´ì˜ ìµœì‹ ì„±ì„ íŒë‹¨í•˜ëŠ” ê¸°ì¤€ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

[í•„ìˆ˜ ì¤€ìˆ˜ ì‚¬í•­]
1. ì£¼ì œ ì§‘ì¤‘: ì˜¤ì§ ì œê³µëœ í•˜ë‚˜ì˜ í‚¤ì›Œë“œì— ëŒ€í•´ì„œë§Œ ê¹Šì´ ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”.
2. ë‚ ì§œ ì–¸ê¸‰ ê¸ˆì§€: ë³¸ë¬¸ ë‚´ì— 'ì˜¤ëŠ˜ì€ {current_date}ì…ë‹ˆë‹¤' í˜¹ì€ 'ì˜¤ëŠ˜'ê³¼ ê°™ì€ êµ¬ì²´ì ì¸ ë‚ ì§œ í‘œí˜„ì„ ì§ì ‘ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”. 
3. ì¸ì‚¬ë§ ê¸ˆì§€: ë„ì…ë¶€ ìê¸°ì†Œê°œë‚˜ ë…ì ì¸ì‚¬ë¥¼ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”. ë°”ë¡œ ë³¸ë¡ ì˜ ì •ë³´ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
4. ë¶„ëŸ‰: ê³µë°± ì œì™¸ 3,000ì ì´ìƒì˜ ë§¤ìš° ìƒì„¸í•œ ë‚´ìš©ì„ ì‘ì„±í•˜ì„¸ìš”. 
5. êµ¬í…ë² ë¥´í¬ ë¸”ë¡ í˜•ì‹: ì›Œë“œí”„ë ˆìŠ¤ ì—ë””í„°ê°€ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ HTML ì£¼ì„ ë¸”ë¡ì„ ì •í™•í•˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”.
   - ì˜ˆ: <!-- wp:heading {{"level":2}} --><h2>ì†Œì£¼ì œ</h2><!-- /wp:heading -->
   - ì˜ˆ: <!-- wp:paragraph --><p>ë‚´ìš©...</p><!-- /wp:paragraph -->
6. SEO ì œëª©: ë§¤ë ¥ì ì´ê³  ê²€ìƒ‰ì— ìœ ë¦¬í•œ ì œëª©ì„ ìƒˆë¡œ ë§Œë“œì„¸ìš”.
7. íƒœê·¸: ê´€ë ¨ íƒœê·¸ 5ê°œë¥¼ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ìƒì„±í•˜ì„¸ìš”.
"""
    
    user_query = f"""
ì›ë³¸ í‚¤ì›Œë“œ: {raw_keyword}

ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "title": "SEO ìµœì í™” ì œëª©",
  "content": "êµ¬í…ë² ë¥´í¬ ë¸”ë¡ í˜•ì‹ì´ ì ìš©ëœ 3,000ì ì´ìƒì˜ ë³¸ë¬¸",
  "excerpt": "í•µì‹¬ ìš”ì•½ 1~2ë¬¸ì¥",
  "tags": "íƒœê·¸1,íƒœê·¸2,íƒœê·¸3,íƒœê·¸4,íƒœê·¸5"
}}
"""
    
    payload = {
        "contents": [{"parts": [{"text": user_query}]}],
        "systemInstruction": {"parts": [{"text": system_prompt}]},
        "generationConfig": {
            "responseMimeType": "application/json"
        }
    }
    
    delays = [1, 2, 4, 8, 16]
    for delay in delays:
        try:
            response = requests.post(url, json=payload, timeout=120)
            if response.status_code == 200:
                result = response.json()
                text_content = result['candidates'][0]['content']['parts'][0]['text']
                clean_json = re.sub(r'^```json\s*|\s*```$', '', text_content.strip(), flags=re.MULTILINE)
                return json.loads(clean_json)
            elif response.status_code in [429, 500, 502, 503, 504]:
                time.sleep(delay)
                continue
            else:
                print(f"API ì˜¤ë¥˜: {response.status_code}", flush=True)
                break
        except Exception as e:
            print(f"ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}", flush=True)
            time.sleep(delay)
            continue
    return None

def get_or_create_tags(base_url, headers, tag_names_str):
    """íƒœê·¸ ì´ë¦„ì„ IDë¡œ ë³€í™˜ (ì—†ìœ¼ë©´ ìƒì„±)"""
    if not tag_names_str:
        return []
    
    tag_names = [t.strip() for t in tag_names_str.split(',') if t.strip()]
    tag_ids = []
    
    for name in tag_names:
        try:
            # 1. ê¸°ì¡´ íƒœê·¸ ê²€ìƒ‰
            search_url = f"{base_url}/wp-json/wp/v2/tags?search={name}"
            res = requests.get(search_url, headers=headers, timeout=10)
            existing_tags = res.json()
            
            found = False
            if isinstance(existing_tags, list):
                for et in existing_tags:
                    if et['name'] == name:
                        tag_ids.append(et['id'])
                        found = True
                        break
            
            if not found:
                # 2. íƒœê·¸ê°€ ì—†ìœ¼ë©´ ìƒì„±
                create_url = f"{base_url}/wp-json/wp/v2/tags"
                create_res = requests.post(create_url, headers=headers, json={"name": name}, timeout=10)
                if create_res.status_code in [200, 201]:
                    tag_ids.append(create_res.json()['id'])
                elif create_res.status_code == 400: # ì´ë¯¸ ì¡´ì¬í•˜ì§€ë§Œ ê²€ìƒ‰ì— ì•ˆê±¸ë¦° ê²½ìš° ë“±
                    # ë‹¤ì‹œ í•œë²ˆ ê²€ìƒ‰ ì‹œë„
                    res = requests.get(search_url, headers=headers, timeout=10)
                    if res.status_code == 200 and res.json():
                        tag_ids.append(res.json()[0]['id'])

        except Exception as e:
            print(f"íƒœê·¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({name}): {e}", flush=True)
            
    return tag_ids

def post_to_wp(content_data):
    """ì›Œë“œí”„ë ˆìŠ¤ REST API ì—…ë¡œë“œ (íƒœê·¸ ID í•„ë“œ ì‚¬ìš©)"""
    base_url = WP_BASE_URL.rstrip('/')
    url = f"{base_url}/wp-json/wp/v2/posts"
    
    auth_str = f"{WP_USERNAME}:{WP_APP_PASSWORD}"
    encoded_auth = base64.b64encode(auth_str.encode('utf-8')).decode('utf-8')
    
    headers = {
        "Authorization": f"Basic {encoded_auth}",
        "Content-Type": "application/json"
    }

    # íƒœê·¸ ì´ë¦„ì„ ID ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    tag_ids = get_or_create_tags(base_url, headers, content_data.get('tags', ''))

    payload = {
        "title": content_data.get('title', ''),
        "content": content_data.get('content', ''),
        "excerpt": content_data.get('excerpt', ''),
        "tags": tag_ids, # ì›Œë“œí”„ë ˆìŠ¤ íƒœê·¸ ì…ë ¥ í•„ë“œì— ì ìš©
        "status": "publish"
    }
    
    try:
        res = requests.post(url, headers=headers, json=payload, timeout=30)
        if res.status_code == 201:
            return True
        else:
            print(f"âš ï¸ ì›Œë“œí”„ë ˆìŠ¤ ì‘ë‹µ ì˜¤ë¥˜: {res.status_code} - {res.text}", flush=True)
            return False
    except Exception as e:
        print(f"â— ì›Œë“œí”„ë ˆìŠ¤ ì—°ê²° ì˜ˆì™¸: {e}", flush=True)
        return False

def main():
    if not WP_USERNAME or not WP_APP_PASSWORD:
        print("âŒ ì¸ì¦ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.", flush=True)
        return

    scraper = NaverScraper()
    print("ğŸš€ [1ë‹¨ê³„] í‚¤ì›Œë“œ ìˆ˜ì§‘ ë° ì •ì œ ì‹œì‘...", flush=True)
    
    jobs = [
        ("101", "ê²½ì œ/ë¹„ì¦ˆë‹ˆìŠ¤"),
        ("105", "IT/í…Œí¬"),
        ("103", "íŒ¨ì…˜/ë·°í‹°/ë¦¬ë¹™"),
        (None, "ì¼ë°˜/ìƒí™œ")
    ]
    
    candidates = []
    for sid, cat in jobs:
        titles = scraper.get_news_ranking(sid) if sid else scraper.get_blog_hot_topics()
        for t in titles[:5]:
            candidates.append({"kw": t, "cat": cat})
        time.sleep(1)

    if not candidates:
        print("âŒ ìˆ˜ì§‘ëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.", flush=True)
        return
        
    if IS_TEST:
        print("\nğŸ§ª [í…ŒìŠ¤íŠ¸ ëª¨ë“œ] 1ê°œ ì¦‰ì‹œ ë°œí–‰ ì‹œë„", flush=True)
        selected = random.sample(candidates, 1)
        posting_times = [0]
    else:
        selected = random.sample(candidates, min(len(candidates), 10))
        total_seconds = 2 * 60 * 60
        posting_times = sorted([random.randint(0, total_seconds) for _ in range(len(selected))])

    last_wait = 0
    for i, item in enumerate(selected):
        wait_for_next = posting_times[i] - last_wait
        if wait_for_next > 0:
            print(f"\nâ³ ëŒ€ê¸°: {wait_for_next//60}ë¶„...", flush=True)
            time.sleep(wait_for_next)
        
        print(f"ğŸ“ ì½˜í…ì¸  ë¶„ì„ ë° ìƒì„± ì¤‘: {item['kw']}", flush=True)
        content_data = generate_content(item['kw'], item['cat'])
        
        if content_data and content_data.get('title'):
            print(f"ğŸ“Œ ìµœì¢… ì œëª©: {content_data['title']}", flush=True)
            if post_to_wp(content_data):
                print(f"âœ… ë°œí–‰ ì™„ë£Œ: {content_data['title']}", flush=True)
            else:
                print(f"âŒ ì›Œë“œí”„ë ˆìŠ¤ ë°œí–‰ ì‹¤íŒ¨", flush=True)
        else:
            print(f"âŒ AI ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨", flush=True)
            
        last_wait = posting_times[i]

    print("\nğŸ‰ ëª¨ë“  ìë™ í¬ìŠ¤íŒ… ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.", flush=True)

if __name__ == "__main__":
    main()
